{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a829d906-cb3e-419c-82e4-9b4eb8cb22d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from typing import Tuple, List, Callable\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pdb\n",
    "import time\n",
    "\n",
    "torch.cuda.is_available() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ff93de-cf2a-48c6-98c3-af0ef02647d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_ids = [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086accbd-e1b8-4930-87e2-b61c6319ca58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from run import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c645565-ebfe-4b57-a96a-e3963d3e93a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TIME_LIMITS and LRs are for simulatiom arguments \n",
    "TIME_LIMITS = {'mnist': 500, 'fashion mnist': 10000  , 'cifar 10': 500, 'celeba': 1500}\n",
    "LRs         = {'mnist': 0.01, 'fashion mnist': 0.001, 'cifar 10': 0.01, 'celeba': 0.001}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa108fb-0519-4ef9-bdf3-caa4991ce419",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'fashion mnist'\n",
    "lr = LRs[dataset_name]\n",
    "time_limit = TIME_LIMITS[dataset_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97b68ea-3b26-4ad9-8f0a-cbb49069a471",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Here you should define different setups that you want to run and compare.\n",
    "### Each config should be added to the \"setups\" dictionary. Config arguments are difined \n",
    "### by a dictionary from argument_name to value. \n",
    " \n",
    "setups = {}\n",
    " \n",
    "group_count = 5\n",
    "client_count = 20\n",
    "max_local_steps = 5\n",
    "\n",
    "server_interaction_time = 10\n",
    "server_waiting_time = 0\n",
    "\n",
    "\n",
    "# Baseline config, one client interacting with the server as if the model is getting trained on the server\n",
    "method = \"identity\"\n",
    "quantizer = {\"method\": method}\n",
    "setups[f\"Baseline\"] = {'algorithm': \"Fed_Avg\", \n",
    "                                     'client count': 1,\n",
    "                                     'local step': 1,\n",
    "                                     'group count': 1,\n",
    "                                     'quantizer': quantizer, \n",
    "                                     'time_limit': time_limit,\n",
    "                                     'lr': lr,\n",
    "                                     'sit': 0,\n",
    "                                     'gpu_ids': gpu_ids }\n",
    "\n",
    "\n",
    "# # # Fed-Avg config example\n",
    "method = \"identity\"\n",
    "quantizer = {\"method\": method}\n",
    "\n",
    "local_steps = max_local_steps\n",
    "setups[f\"Fed-Avg ({client_count},{group_count},{local_steps}) sit: {server_interaction_time}\"] = {'algorithm': \"Fed_Avg\", \n",
    "                                                                                         'client count': client_count,\n",
    "                                                                                         'local step': local_steps,\n",
    "                                                                                         'group count': group_count,\n",
    "                                                                                         'quantizer': quantizer, \n",
    "                                                                                         'time_limit': time_limit,\n",
    "                                                                                         'lr': lr,\n",
    "                                                                                         'sit': server_interaction_time,\n",
    "                                                                                         'gpu_ids': gpu_ids}\n",
    "\n",
    "# # # ## QuAFL config example\n",
    "method = \"lattice\"\n",
    "quant_q = 14\n",
    "server_interaction_time *= quant_q / 32\n",
    "server_interaction_time = float(\"{0:.3f}\".format(server_interaction_time))\n",
    "quantizer = {\"method\": method, 'quant_q': quant_q, 'quant_s': 0.0001}#, 'quant_q': quant_q, 'quant_s': 0.001\n",
    "# method = \"qsgd\"\n",
    "# q_levels = 16\n",
    "# server_interaction_time *= 4 / 32\n",
    "# server_interaction_time = float(\"{0:.3f}\".format(server_interaction_time))\n",
    "# quantizer = {\"method\": method, 'k': q_levels}\n",
    "setups[f\"QuAFL   ({client_count},{group_count},{max_local_steps},{method}) swt: {server_waiting_time} sit: {server_interaction_time}\"] = {'algorithm': \"quantized_fl\", \n",
    "                                                                                          'client count': client_count,\n",
    "                                                                                          'local step': max_local_steps,\n",
    "                                                                                          'group count': group_count,\n",
    "                                                                                          'quantizer': quantizer, \n",
    "                                                                                          'time_limit': time_limit,\n",
    "                                                                                          'lr': lr,\n",
    "                                                                                          'swt': server_waiting_time,\n",
    "                                                                                          'sit': server_interaction_time,\n",
    "                                                                                          'gpu_ids': gpu_ids}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Fed-Buff config example\n",
    "method = \"identity\"\n",
    "quantizer = {\"method\": method}\n",
    "\n",
    "local_steps = max_local_steps\n",
    "setups[f\"FedBuff ({client_count},{group_count},{local_steps}) sit: {server_interaction_time}\"] = {'algorithm': \"FedBuff\", \n",
    "                                                                                         'client count': client_count,\n",
    "                                                                                         'local step': local_steps,\n",
    "                                                                                         'group count': group_count,\n",
    "                                                                                         'quantizer': quantizer, \n",
    "                                                                                         'time_limit': time_limit,\n",
    "                                                                                         'lr': lr,\n",
    "                                                                                         'sit': server_interaction_time,\n",
    "                                                                                         'gpu_ids': gpu_ids}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48954e91-ea69-468b-9588-4db1adbf93fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "log_period = 200 ## Simulation time difference between loggings.\n",
    "logs, trainers = run(setups, dataset_name, log_period, count=client_count, decreasing=False, slow_client_ratio = 0.50)\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Finished in {end - start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373810fc-7cbe-4a34-8fa5-34c43e8358ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_axis, y_axis = \"Local steps\", \"Loss\"\n",
    "plot_trends(logs, x_axis, y_axis, 0)\n",
    "\n",
    "x_axis, y_axis = \"Server steps\", \"Loss\"\n",
    "plot_trends(logs, x_axis, y_axis, 0)\n",
    "\n",
    "x_axis, y_axis = \"Time\", \"Loss\"\n",
    "plot_trends(logs, x_axis, y_axis, 0) \n",
    "\n",
    "x_axis, y_axis = \"Aggregated local steps\", \"Loss\"\n",
    "plot_trends(logs, x_axis, y_axis, 0) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd7ec83",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis, y_axis = \"Local steps\", \"Accuracy\"\n",
    "plot_trends(logs, x_axis, y_axis)\n",
    "\n",
    "x_axis, y_axis = \"Server steps\", \"Accuracy\"\n",
    "plot_trends(logs, x_axis, y_axis)\n",
    "\n",
    "x_axis, y_axis = \"Time\", \"Accuracy\"\n",
    "plot_trends(logs, x_axis, y_axis)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
